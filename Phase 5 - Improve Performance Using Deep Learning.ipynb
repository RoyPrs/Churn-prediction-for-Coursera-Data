{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13a44c3-4e9e-4ce1-bb7b-57df8647231a",
   "metadata": {},
   "source": [
    "# Phase 5 - Improve Performance Using Feature Engineering\n",
    "\n",
    "In this notebook, I am going to predict churn using feature selection. Selecting features will be an iterative process; I will train an SVM model, list features by importance, and remove the least important feature. Then, I will train the model using n-1 features. This loop ends when we have only one feature remained. Adding features to a FIFO stack, along the way, gives us the list of features in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d4f99-5bf1-48ab-8770-93eeb597ae6d",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdff5398-e8c8-4f45-875a-34647d58be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python packages\n",
    "from math import sqrt\n",
    "import pickle\n",
    "\n",
    "# Data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Packages\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Data preprocessing packages\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "roc_auc_score, roc_curve, auc,\\\n",
    "confusion_matrix, classification_report,\\\n",
    "ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe2207e-e8c2-4695-8588-9c86f2b243ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adak\\AppData\\Local\\Temp\\ipykernel_1364\\1918148274.py:1: DtypeWarning: Columns (6,7,25,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"Data/Coursera.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(413955, 37)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/Coursera.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f1e808-f3d8-40ef-88d8-6ad4aa51be5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_professional_certificate                object\n",
       "is_gateway_certificate                     object\n",
       "is_subscription_started_with_free_trial    object\n",
       "is_active_capstone_during_pay_period       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.iloc[[6,7,25,31]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6707a609-59b3-4553-b07b-f9f492b9077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column is_professional_certificate: [True False nan]\n",
      "Unique values in column is_gateway_certificate: [True False nan]\n",
      "Unique values in column is_subscription_started_with_free_trial: [False True nan]\n",
      "Unique values in column is_active_capstone_during_pay_period: [False True nan]\n"
     ]
    }
   ],
   "source": [
    "for i in data.dtypes.iloc[[6,7,25,31]].index:\n",
    "    print(f\"Unique values in column {i}: {data[i].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1377df-e5fc-4e14-8311-702f24bdf693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates (if any)\n",
    "data.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b21702b-b6a5-4763-b14b-95cbef2c499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413953, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove nans\n",
    "data = data.dropna()\n",
    "# data.isna().sum()\n",
    "# df0.info()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ee80cd-42a6-4e20-ae61-df65c1911a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps\n",
    "def preprocess_data(df):    \n",
    "\n",
    "    # drop null rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    # remove highly correlated variables\n",
    "    df = df.drop([ 'days_til_next_payment_due', # correlated with 'days_since_last_payment'\n",
    "                   'specialization_id',\n",
    "                   'subscription_id', 'observation_dt'\n",
    "                   ], axis=1)\n",
    "\n",
    "    # map learner country groups by geography\n",
    "    # create a dictionary to map learner country groups\n",
    "    country_map = {'Northern Europe': 'Europe',\n",
    "                    'Australia and New Zealand': 'ANZ',\n",
    "                    'United States': 'NorthAm',\n",
    "                    'India': 'Asia',\n",
    "                    'East Asia': 'Asia',\n",
    "                        'Eastern Europe': 'Europe',\n",
    "                        'Southern Europe': 'Europe',\n",
    "                        'Southeast Asia': 'Asia', \n",
    "                        'Middle East': 'MENA',\n",
    "                        'Africa and developing Middle East': 'MENA',\n",
    "                        'China': 'Asia', \n",
    "                        'Canada': 'NorthAm',\n",
    "                        'Non-Brazil Latin America': 'LatAm', \n",
    "                        'Brazil': 'LatAm',\n",
    "                        'Russia and neighbors': 'Europe'}\n",
    "\n",
    "    # map the learner country groups\n",
    "    df['learner_country_group'] = df['learner_country_group'].map(country_map)\n",
    "\n",
    "    # group 'other' gender into 'unknown'\n",
    "    df['learner_gender'] = df['learner_gender'].replace('other', 'unknown')\n",
    "    # df['learner_gender'] = df['learner_gender'].cat.rename_categories(\n",
    "    #     {'other': 'unknown','female':'female', 'male':'male', 'unknown':'unknown'})\n",
    "    \n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # create new feature, 'pct_learner_paid_active'\n",
    "    try:\n",
    "        df['pct_learner_paid_active'] = df['learner_cnt_other_courses_paid_active'] / df['learner_cnt_other_courses_active']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_learner_paid_active']=0\n",
    "\n",
    "    # create new feature, 'pct_learner_paid_items_completed'\n",
    "    try:\n",
    "        df['pct_learner_paid_items_completed'] = df['learner_cnt_other_courses_paid_items_completed'] / df['learner_cnt_other_courses_items_completed']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_learner_paid_items_completed']=0\n",
    "\n",
    "    # create new feature, 'revenue_per_transaction'\n",
    "    try:\n",
    "        df['revenue_per_transaction'] = df['learner_other_revenue'] / df['learner_cnt_other_transactions_past']\n",
    "    except ZeroDivisionError:\n",
    "        df['revenue_per_transaction']=0\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # create new feature, 'pct_enrollments_active_before_payment_period'\n",
    "    try:\n",
    "        df['pct_enrollments_active_before_payment_period'] = df['cnt_enrollments_active_before_payment_period'] / df['cnt_enrollments_started_before_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_enrollments_active_before_payment_period']=0\n",
    "    \n",
    "    # create new feature, 'pct_enrollments_completed_before_payment_period'\n",
    "    try:\n",
    "        df['pct_enrollments_completed_before_payment_period'] = df['cnt_enrollments_completed_before_payment_period'] / df['cnt_enrollments_started_before_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_enrollments_completed_before_payment_period']=0\n",
    "\n",
    "    # create new feature, 'pct graded items completed before payment period'\n",
    "    try:\n",
    "        df['pct_graded_items_completed_before_payment_period'] = df['cnt_graded_items_completed_before_payment_period'] / df['cnt_items_completed_before_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_graded_items_completed_before_payment_period']=0\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # create new feature, 'pct_enrollments_active_during_payment_period'\n",
    "    try:\n",
    "        df['pct_enrollments_active_during_payment_period'] = df['cnt_enrollments_active_during_payment_period'] / df['cnt_enrollments_started_during_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_enrollments_active_during_payment_period']=0\n",
    "        \n",
    "    # create new feature, 'pct_enrollments_completed_during_payment_period'\n",
    "    try:\n",
    "        df['pct_enrollments_completed_during_payment_period'] = df['cnt_enrollments_completed_during_payment_period'] / df['cnt_enrollments_started_during_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_enrollments_completed_during_payment_period']=0\n",
    "\n",
    "    # create new feature, 'pct_graded_items_completed_during_payment_period'\n",
    "    try:\n",
    "        df['pct_graded_items_completed_during_payment_period'] = df['cnt_graded_items_completed_during_payment_period'] / df['cnt_items_completed_during_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['pct_graded_items_completed_during_payment_period']=0\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # create new feature, 'hrs_per_day_active_before_payment_period'\n",
    "    try:\n",
    "        df['hrs_per_day_active_before_payment_period'] = df['sum_hours_learning_before_payment_period'] / df['cnt_days_active_before_payment_period']\n",
    "    except ZeroDivisionError:\n",
    "        df['hrs_per_day_active_before_payment_period']=0\n",
    "\n",
    "    # create new feature, 'hrs_per_day_active_during_payment_period'\n",
    "    try:\n",
    "        df['hrs_per_day_active_during_payment_period'] = df['sum_hours_learning_during_payment_period'] / df['cnt_days_active_during_payment_period']   \n",
    "    except ZeroDivisionError:\n",
    "        df['hrs_per_day_active_during_payment_period']=0\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # drop columns that are no longer needed\n",
    "    df = df.drop(['learner_cnt_other_courses_active',\n",
    "                  'learner_cnt_other_courses_paid_active',\n",
    "                  'learner_cnt_other_courses_items_completed',\n",
    "                  'learner_cnt_other_courses_paid_items_completed',\n",
    "                  'learner_cnt_other_transactions_past', \n",
    "                  'learner_other_revenue',\n",
    "                 'cnt_enrollments_started_before_payment_period',\n",
    "                 'cnt_enrollments_completed_before_payment_period',\n",
    "                 'cnt_enrollments_active_before_payment_period',\n",
    "                 'cnt_items_completed_before_payment_period',\n",
    "                 'cnt_graded_items_completed_before_payment_period',\n",
    "                 'cnt_enrollments_started_during_payment_period',\n",
    "                 'cnt_enrollments_completed_during_payment_period',\n",
    "                 'cnt_enrollments_active_during_payment_period',\n",
    "                 'cnt_items_completed_during_payment_period',\n",
    "                 'cnt_graded_items_completed_during_payment_period',\n",
    "                 'sum_hours_learning_before_payment_period',\n",
    "                 'sum_hours_learning_during_payment_period',\n",
    "                 'cnt_days_active_before_payment_period',\n",
    "                 'cnt_days_active_during_payment_period',\n",
    "    ], axis=1)\n",
    "\n",
    "    \n",
    "    # define the columns to convert to boolean\n",
    "    cols_to_convert = ['is_professional_certificate', 'is_gateway_certificate', \n",
    "                       'is_subscription_started_with_free_trial', 'is_active_capstone_during_pay_period']\n",
    "\n",
    "    # write a function to convert columns into boolean\n",
    "    def convert_to_boolean(df, col):\n",
    "        df[col] = df[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # convert columns to boolean\n",
    "    for col in cols_to_convert:\n",
    "        convert_to_boolean(df, col)\n",
    "\n",
    "    # map subscription period order of more than 4 into 4\n",
    "    df['subscription_period_order'] = df['subscription_period_order'].apply(lambda x: 4 if x > 4 else x)\n",
    "\n",
    "    # convert subscription period order to categorical\n",
    "    df['subscription_period_order'] = df['subscription_period_order'].astype('category')\n",
    "\n",
    "    # get categorical columns\n",
    "    cat_cols = ['specialization_domain', 'subscription_period_order', 'learner_country_group', 'learner_gender']\n",
    "\n",
    "    # get dummies for categorical columns\n",
    "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    # fill null values with 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # replace inf with 0\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # return the preprocessed dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b27b6ff-c606-4646-8700-e1cea8628e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413953, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_data(data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8989cdbd-7d05-4fb8-9198-7df3c71eec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('Pickles//data.pickle', 'wb') as file:\n",
    "    pickle.dump(df, file) \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29eb547-2a64-45e7-a669-b9ff1bb2b46f",
   "metadata": {},
   "source": [
    "## Sample the Data\n",
    "Since our dataset is relatively large, it takes very long to train a SVM model. So, I sample a smaller chunck of the data to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9c0c07c-3549-4f27-93bb-6c0c67193ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 40)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "df = df.sample(n, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5adf20eb-d281-4aa4-aac0-3c7906b63f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['is_retained'])\n",
    "y = df['is_retained']\n",
    "\n",
    "# # Split into train and validate sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9f7ba29d-aeb3-42de-909e-f1a41358d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done SVC kernel poly\n",
      "Train Precision: 0.8081717451523546\n",
      "Test Precision: 0.6825817860300619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = Pipeline([\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", SVC(kernel=\"poly\", degree=4, coef0=2, C=0.085))\n",
    "    # (\"svm_clf\", SVC(kernel=\"rbf\", gamma=0.5, C=20)) \n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Done SVC kernel poly')\n",
    "\n",
    "yt = clf.predict(X_train)\n",
    "print(\"Train Precision:\", precision_score(yt, y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e774000-28fc-4617-b931-24b51fbf2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done SVC kernel rbf\n",
      "Train Precision: 0.734533702677747\n",
      "Test Precision: 0.7012522361359571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = Pipeline([\n",
    " (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=.0035, C=2)) \n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Done SVC kernel rbf')\n",
    "\n",
    "yt = clf.predict(X_train)\n",
    "print(\"Train Precision:\", precision_score(yt, y_train))\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebdc7253-a1a2-4c1f-9261-a7343e92a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision: 0.9997691597414589\n",
      "Test Precision: 0.6942909760589319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=1000,\n",
    " max_samples=5000, bootstrap=False, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "yt = bag_clf.predict(X_train)\n",
    "print(\"Train Precision:\", precision_score(yt, y_train))\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbb45a-153d-4854-ac43-31567aec33f8",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db428ff2-6e40-4c1c-9b7e-9e0606d98198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
